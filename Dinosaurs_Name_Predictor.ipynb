{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character level language model - Dinosaurus land\n",
    "\n",
    "The program to predict Dinosaur names based on the custom build `RNN class`. The [dataset](data/dinos.txt) used for training is the names of actual Dinosaurs as in the file `dinos.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from util.RNN_Generator import *\n",
    "from util.helper_functions import *\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Loading the Dataset\n",
    "\n",
    "- for simplification, everything is first converted to lower characters\n",
    "- list of characters are created to later create simple embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19909 total characters and 27 unique characters in your data.\n"
     ]
    }
   ],
   "source": [
    "data = open('.\\data\\dinos.txt', 'r').read()\n",
    "data= data.lower()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are in total 26 characters and also the new line character \"\\n\" is added in the list of characters which can also be infered as a `<EOS>` (End of Sentence, but here word).\n",
    "Two dictionary embeddings `char_to_ix` and `ix_to_char` are the created as python dictionaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of the model\n",
    "\n",
    "Your model will have the following structure: \n",
    "\n",
    "- Initialize parameters \n",
    "- Run the optimization loop\n",
    "    - Forward propagation to compute the loss function\n",
    "    - Backward propagation to compute the gradients with respect to the loss function\n",
    "    - Clip the gradients to avoid exploding gradients\n",
    "    - Using the gradients, update your parameter with the gradient descent update rule.\n",
    "- Return the learned parameters \n",
    "    \n",
    "<img src=\"images/rnn.png\" style=\"width:450;height:300px;\">\n",
    "<caption><center> **Figure 1**: Recurrent Neural Network, similar to what you had built in the previous notebook \"Building a RNN - Step by Step\".  </center></caption>\n",
    "\n",
    "At each time-step, the RNN tries to predict what is the next character given the previous characters. The dataset $X = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, ..., x^{\\langle T_x \\rangle})$ is a list of characters in the training set, while $Y = (y^{\\langle 1 \\rangle}, y^{\\langle 2 \\rangle}, ..., y^{\\langle T_x \\rangle})$ is such that at every time-step $t$, we have $y^{\\langle t \\rangle} = x^{\\langle t+1 \\rangle}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Training the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First the RNN instance is created\n",
    "- Then the model is trained for specific epochs\n",
    "- Dinsosaurs after specific epochs is as displayed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_a, n_x, n_y = 50, vocab_size, vocab_size\n",
    "epochs = 40000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss: 23.087336\n",
      "\n",
      "Nkzxwtdmfqoeyhsqwasjkjvu\n",
      "Kneb\n",
      "Kzxwtdmfqoeyhsqwasjkjvu\n",
      "Neb\n",
      "Zxwtdmfqoeyhsqwasjkjvu\n",
      "Eb\n",
      "Xwtdmfqoeyhsqwasjkjvu\n",
      "\n",
      "\n",
      "Iteration: 2000, Loss: 27.884160\n",
      "\n",
      "Liusskeomnolxeros\n",
      "Hmdaairus\n",
      "Hytroligoraurus\n",
      "Lecalosapaus\n",
      "Xusicikoraurus\n",
      "Abalpsamantisaurus\n",
      "Tpraneronxeros\n",
      "\n",
      "\n",
      "Iteration: 4000, Loss: 25.901815\n",
      "\n",
      "Mivrosaurus\n",
      "Inee\n",
      "Ivtroplisaurus\n",
      "Mbaaisaurus\n",
      "Wusichisaurus\n",
      "Cabaselachus\n",
      "Toraperlethosdarenitochusthiamamumamaon\n",
      "\n",
      "\n",
      "Iteration: 6000, Loss: 24.608779\n",
      "\n",
      "Onwusceomosaurus\n",
      "Lieeaerosaurus\n",
      "Lxussaurus\n",
      "Oma\n",
      "Xusteonosaurus\n",
      "Eeahosaurus\n",
      "Toreonosaurus\n",
      "\n",
      "\n",
      "Iteration: 8000, Loss: 24.070350\n",
      "\n",
      "Onxusichepriuon\n",
      "Kilabersaurus\n",
      "Lutrodon\n",
      "Omaaerosaurus\n",
      "Xutrcheps\n",
      "Edaksoje\n",
      "Trodiktonus\n",
      "\n",
      "\n",
      "Iteration: 10000, Loss: 23.844446\n",
      "\n",
      "Onyusaurus\n",
      "Klecalosaurus\n",
      "Lustodon\n",
      "Ola\n",
      "Xusodonia\n",
      "Eeaeosaurus\n",
      "Troceosaurus\n",
      "\n",
      "\n",
      "Iteration: 12000, Loss: 23.291971\n",
      "\n",
      "Onyxosaurus\n",
      "Kica\n",
      "Lustrepiosaurus\n",
      "Olaagrraiansaurus\n",
      "Yuspangosaurus\n",
      "Eealosaurus\n",
      "Trognesaurus\n",
      "\n",
      "\n",
      "Iteration: 14000, Loss: 23.382338\n",
      "\n",
      "Meutromodromurus\n",
      "Inda\n",
      "Iutroinatorsaurus\n",
      "Maca\n",
      "Yusteratoptititan\n",
      "Ca\n",
      "Troclosaurus\n",
      "\n",
      "\n",
      "Iteration: 16000, Loss: 23.274687\n",
      "\n",
      "Niwusedia\n",
      "Inda\n",
      "Kustichaptes\n",
      "Necalosaurus\n",
      "Ytrodon\n",
      "Daahosaurus\n",
      "Trodon\n",
      "\n",
      "\n",
      "Iteration: 18000, Loss: 22.852921\n",
      "\n",
      "Phyushis\n",
      "Melaa\n",
      "Myssodon\n",
      "Pegalosaurus\n",
      "Yusichis\n",
      "Eg\n",
      "Trodonosaurus\n",
      "\n",
      "\n",
      "Iteration: 20000, Loss: 22.983534\n",
      "\n",
      "Meustroraptor\n",
      "Lolaa\n",
      "Lyusaurus\n",
      "Ngbagpklachus\n",
      "Ystolomoneylus\n",
      "Ejaisoh\n",
      "Trrchephasaurus\n",
      "\n",
      "\n",
      "Iteration: 22000, Loss: 22.717269\n",
      "\n",
      "Piutysaurus\n",
      "Migaa\n",
      "Mustolodon\n",
      "Pedadosaurus\n",
      "Ytroenator\n",
      "Gaberrgaadrithanus\n",
      "Trodon\n",
      "\n",
      "\n",
      "Iteration: 24000, Loss: 22.641291\n",
      "\n",
      "Meustolongiangris\n",
      "Indaachuacerston\n",
      "Jvrosaurus\n",
      "Mecaisin\n",
      "Yuronlerantasaurus\n",
      "Daacosaurus\n",
      "Trodonosaurus\n",
      "\n",
      "\n",
      "Iteration: 26000, Loss: 22.672323\n",
      "\n",
      "Nivusmangoraurosaurus\n",
      "Kleeadon\n",
      "Luutodon\n",
      "Ola\n",
      "Yustanitiatesaurus\n",
      "Eebhosaurus\n",
      "Trtengoraurus\n",
      "\n",
      "\n",
      "Iteration: 28000, Loss: 22.531583\n",
      "\n",
      "Omus\n",
      "Llacaestaechuitalkus\n",
      "Lytrodon\n",
      "Olaadisan\n",
      "Ytroceosaurus\n",
      "Eiadosaurus\n",
      "Trtarasaurus\n",
      "\n",
      "\n",
      "Iteration: 30000, Loss: 22.538524\n",
      "\n",
      "Onyxauiodop\n",
      "Lodaceria\n",
      "Lytrodon\n",
      "Olaabus\n",
      "Yushangosaurus\n",
      "Efadosaurus\n",
      "Trocomisaurus\n",
      "\n",
      "\n",
      "Iteration: 32000, Loss: 22.265772\n",
      "\n",
      "Nivusceodilasaurus\n",
      "Llacanithabosaurus\n",
      "Lysroililiaurosaurus\n",
      "Necamisaurus\n",
      "Yusianchmatisaurus\n",
      "Ehamosaurus\n",
      "Trtanasaurus\n",
      "\n",
      "\n",
      "Iteration: 34000, Loss: 22.379533\n",
      "\n",
      "Mawsosaurus\n",
      "Jiabaisaurus\n",
      "Kuspianaheita\n",
      "Macagposaurus\n",
      "Yssiangnhathus\n",
      "Eiaeosaurus\n",
      "Trolonosaurus\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnn_model = RNN(n_a, n_x, n_y)\n",
    "rnn_model.model(data, ix_to_char, char_to_ix, num_iterations = epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "The simple RNN model created reasonable names but as the model was character level, RNN produced good results.\n",
    "But when we want to generate word level model, RNN are not a good choice, but rather `LSTM` are used to generate sequential models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "This project was a project undertaken by me as a part of the Coursera course \n",
    "`Deep Learning Specialization` by `Andrew NG`. \n",
    "I would hence like to thank and acknowledge them to help me give better understanding in the sequential modelling by such cool projects. "
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "1dYg0",
   "launcher_item_id": "MLhxP"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
